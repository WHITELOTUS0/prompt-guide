{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Role Prompting\n",
        "\n",
        "Role prompting involves asking the LLM to assume a specific role or identity before performing a given task, such as acting as a copywriter. This can help guide the model's response by providing a context or perspective for the task. To work with role prompts, you could iteratively:\n",
        "\n",
        "- Specify the role in your prompt, e.g., \"As a copywriter, create some attention-grabbing taglines for AWS services.\"\n",
        "- Use the prompt to generate an output from an LLM.\n",
        "- Analyze the generated response and, if necessary, refine the prompt for better results."
      ],
      "metadata": {
        "id": "m3qPzVJCLQOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXHDzupULK0F",
        "outputId": "7c8f0648-4e9b-4811-c2c6-474e479f0efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-openai\n",
        "%pip install -qU langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "# Get the API key from Colab's userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "template = \"\"\"\n",
        "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
        "What's a cool song title for a song about {theme} in the year {year}?\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"theme\", \"year\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Input data for the prompt\n",
        "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "response = chain.invoke(input_data)\n",
        "\n",
        "print(\"Theme: interstellar travel\")\n",
        "print(\"Year: 3030\")\n",
        "print(\"AI-generated song title:\", response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3DiqTFtL3Zf",
        "outputId": "02abbd41-5e22-4808-9d09-5fab667f516d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme: interstellar travel\n",
            "Year: 3030\n",
            "AI-generated song title: \"Galactic Odyssey: Echoes of the Stars\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a good prompt for several reasons:\n",
        "\n",
        "- **Clear instructions**: The prompt is phrased as a clear request for help in generating a song title, and it specifies the context: \"As a futuristic robot band conductor.\" This helps the LLM understand that the desired output should be a song title related to a futuristic scenario.\n",
        "- **Specificity**: The prompt asks for a song title that relates to a specific theme and a specific year, \"{theme} in the year {year}.\" This provides enough context for the LLM to generate a relevant and creative output. The prompt can be adapted for different themes and years by using input variables, making it versatile and reusable.\n",
        "- **Open-ended creativity**: The prompt allows for open-ended creativity, as it doesn't limit the LLM to a particular format or style for the song title. The LLM can generate a diverse range of song titles based on the given theme and year.\n",
        "- **Focus on the task**: The prompt is focused solely on generating a song title, making it easier for the LLM to provide a suitable output without getting sidetracked by unrelated topics.\n",
        "\n",
        "These elements help the LLM understand the user's intention and generate a suitable response."
      ],
      "metadata": {
        "id": "gHs4glA0McPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Shot Prompting\n",
        "Few Shot Prompting In the next example, the LLM is asked to provide the emotion associated with a given color based on a few examples of color-emotion pairs:\n",
        "\n"
      ],
      "metadata": {
        "id": "7B5dkfwGM5Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
        "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
        "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
        "]\n",
        "\n",
        "example_formatter_template = \"\"\"\n",
        "Color: {color}\n",
        "Emotion: {emotion}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"color\", \"emotion\"],\n",
        "    template=example_formatter_template,\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
        "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n",
        "\n",
        "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
        "\n",
        "prompt=PromptTemplate(template=formatted_prompt, input_variables=[])\n",
        "chain = prompt | llm\n",
        "\n",
        "# Run the Runnable to get the AI-generated emotion associated with the input color\n",
        "response = chain.invoke({})\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8_3tsGZNHdx",
        "outputId": "db0ed88b-f38b-482c-d706-3b5472a27bba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color: purple  \n",
            "Emotion: creativity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt provides **clear instructions** and few-shot examples to help the model understand the task."
      ],
      "metadata": {
        "id": "bCSGjlIMOEdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain Prompting\n",
        "Chain Prompting refers to the practice of chaining consecutive prompts, where the output of a previous prompt becomes the input of the successive prompt.\n",
        "\n",
        "To use chain prompting with LangChain, you could:\n",
        "\n",
        "- Extract relevant information from the generated response.\n",
        "- Use the extracted information to create a new prompt that builds upon the previous response.\n",
        "- Repeat steps as needed until the desired output is achieved.\n",
        "\n",
        "`PromptTemplate` class makes constructing prompts with dynamic inputs easier. This is useful when creating a prompt chain that depends on previous answers."
      ],
      "metadata": {
        "id": "rxcOq5rdOS8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity? Give the name only and stop there.\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)\n",
        "\n",
        "# Create the Runnable for the first prompt\n",
        "chain_question = prompt_question | llm\n",
        "\n",
        "response_question = chain_question.invoke({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.content.strip()\n",
        "\n",
        "# Create the Runnable for the second prompt\n",
        "chain_fact = prompt_fact | llm\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the Runnable for the second prompt\n",
        "response_fact = chain_fact.invoke(input_data)\n",
        "\n",
        "print(\"Scientist:\", scientist)\n",
        "print(\"Fact:\", response_fact.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PquSDyVMOt01",
        "outputId": "33524d99-8541-4fc7-8d94-b6a856d635af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scientist: Einstein\n",
            "Fact: Einstein's theory of general relativity, published in 1915, is a fundamental theory of gravitation that describes gravity not as a force, but as a curvature of spacetime caused by mass and energy. According to this theory, massive objects like planets and stars warp the fabric of spacetime around them, and this curvature affects the motion of other objects, causing them to follow curved paths. General relativity has profound implications for our understanding of the universe, including the behavior of light around massive bodies, the expansion of the universe, and the prediction of phenomena such as black holes and gravitational waves. It has been confirmed through numerous experiments and observations, fundamentally changing our understanding of gravity and the cosmos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bad Prompt Example**"
      ],
      "metadata": {
        "id": "7uUeP6vTP1l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something interesting about {scientist}.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = prompt_question |llm\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.invoke({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.content.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = prompt_fact | llm\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.invoke(input_data)\n",
        "\n",
        "print(\"Scientist:\", scientist)\n",
        "print(\"Fact:\", response_fact.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daJ7RFZwPv6X",
        "outputId": "437d2eaa-5913-40ea-a482-2557060ce0e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scientist: The famous scientist who developed the theory of general relativity is Albert Einstein.\n",
            "Fact: One interesting aspect of Albert Einstein's life is that he was not only a brilliant physicist but also a passionate advocate for civil rights. Despite being a prominent figure in the scientific community, Einstein used his platform to speak out against racism and discrimination. He was a member of the NAACP and developed a close friendship with civil rights leader W.E.B. Du Bois. Einstein often expressed his belief in the importance of equality and justice, famously stating, \"Racism is a disease of white people.\" His commitment to social issues extended beyond his scientific work, highlighting his belief that scientists have a responsibility to engage with the world and advocate for positive change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought Prompting\n",
        "Chain of Thought Prompting (CoT) is a technique developed to encourage large language models to explain their reasoning process, leading to more accurate results. By providing few-shot exemplars demonstrating the reasoning process, the LLM is guided to explain its reasoning when answering the prompt. This approach has been found effective in improving results on tasks like arithmetic, common sense, and symbolic reasoning.\n",
        "\n",
        "In the context of LangChain, CoT can be beneficial for several reasons. First, it can help break down complex tasks by assisting the LLM in decomposing a complex task into simpler steps, making it easier to understand and solve the problem. This is particularly useful for calculations, logic, or multi-step reasoning tasks. Second, CoT can guide the model through related prompts, helping generate more coherent and contextually relevant outputs. This can lead to more accurate and useful responses in tasks that require a deep understanding of the problem or domain.\n",
        "\n",
        "There are some limitations to consider when using CoT. One limitation is that it has been found to yield performance gains only when used with models of approximately 100 billion parameters or larger; smaller models tend to produce illogical chains of thought, which can lead to worse accuracy than standard prompting. Another limitation is that CoT may not be equally effective for all tasks. It has been shown to be most effective for tasks involving arithmetic, common sense, and symbolic reasoning. For other types of tasks, the benefits of using CoT might be less pronounced or even counterproductive.\n",
        "\n",
        "**Tips for Effective Prompt Engineering**\n",
        "\n",
        "- **Be specific with your prompt**: Provide enough context and detail to guide the LLM toward the desired output.\n",
        "- **Force conciseness when needed**.\n",
        "- **Encourage the model to explain its reasoning**: This can lead to more accurate results, especially for complex tasks.\n",
        "\n",
        "Keep in mind that prompt engineering is an iterative process, and it may require several refinements to obtain the best possible answer. As LLMs become more integrated into products and services, the ability to create effective prompts will be an important skill to have."
      ],
      "metadata": {
        "id": "eH9rPBPIRb_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the secret to happiness?\",\n",
        "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
        "    }, {\n",
        "        \"query\": \"How can I become more productive?\",\n",
        "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "# Create the Runnable for the few-shot prompt template\n",
        "chain = few_shot_prompt_template | llm\n",
        "\n",
        "# Define the user query\n",
        "user_query = \"What are some tips for improving communication skills?\"\n",
        "\n",
        "# Run the Runnable for the user query\n",
        "response = chain.invoke({\"query\": user_query})\n",
        "\n",
        "print(\"User Query:\", user_query)\n",
        "print(\"AI Response:\", response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stAG8wQARxYP",
        "outputId": "0ebc1c6d-2c56-437e-e52f-e0060b53591a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: What are some tips for improving communication skills?\n",
            "AI Response: Practice active listening, be clear and concise in your messages, and seek feedback to refine your approach. Engaging in conversations regularly and being open to different perspectives can also enhance your skills.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt:\n",
        "\n",
        "- **Provides a clear context in the prefix**: The prompt states that the AI is a life coach providing insightful and practical advice. This context helps guide the AI's responses and ensures they align with the intended purpose.\n",
        "- **Uses examples** that demonstrate the AI's role and the **type of responses** it generates: By providing relevant examples, the AI can better understand the style and tone of the responses it should produce. These examples serve as a reference for the AI to generate similar responses that are consistent with the given context.\n",
        "- Separates examples and the actual query: This **allows the AI to understand the format it should follow**, ensuring a clear distinction between example conversations and the user's input. This separation helps the AI to focus on the current query and respond accordingly.\n",
        "- Includes a clear suffix that indicates where the user's input goes and where the AI should provide its response: The suffix acts as a cue for the AI, showing where the user's query ends and the AI's response should begin. This structure helps maintain a **clear and consistent format** for the generated responses.\n",
        "By using this well-structured prompt, the AI can understand its role, the context, and the expected response format, leading to more accurate and useful outputs."
      ],
      "metadata": {
        "id": "4e8WWNj5SP_V"
      }
    }
  ]
}